---
title: "Project 5 OCR Post-processing Improvment"
output: html_document
---
# Step 1 - Load library and source code

```{r load lib, warning=FALSE, message = FALSE}
packages.used <- c("devtools","vwr","kernlab", "rlist", "utils", "ggplot2","ngram","stringi","qualV","rapportools","xgboost","JOUSBoost")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed)>0) { install.packages(packages.needed, dependencies = TRUE) }
if (!require("pacman")) {
  library(devtools)
  install_github("trinker/pacman")
}
library(vwr)
library(kernlab)
library(rlist)
library(utils)
library(ggplot2)
library(ngram)
library(stringi)
library(qualV)
library(rapportools)
library(xgboost)
library(JOUSBoost)
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)

source('../lib/document_match.R')
source('../lib/features.R')
source('../lib/svm_cross_validation.R')
source('../lib/prob_score.R')
source('../lib/perform_correction.R')
source('../lib/rebuild_lines.R')
source('../lib/count_char.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
```

# Step 2 - read the files and conduct Tesseract OCR

# Step 3 - Error detection
Our error detection improvement was based on the SVM classification, with several added features, particularly context-based. Other more advanced models were carried out in python.

Now, we are ready to conduct post-processing, based on the Tessearct OCR output. First of all, we need to detect errors, or *incorrectly processed words*.

The referenced papers are:

Probabilistic techniques -- [SVM garbage detection](https://dl.acm.org/citation.cfm?doid=2034617.2034626)

[Statistical Learning for OCR Text Correction](https://arxiv.org/pdf/1611.06950.pdf)

[OCR Post-Processing Text Correction using Simulated Annealing](http://aclweb.org/anthology/U17-1015)


## Step 3.1 - Data preperation

First, we prepare the data based on all 100 tesseract files. We label all the tokens as non-error or error through line-by-line comparison, and use the tokens in the line of interest in the ground truth files as a dictionary to check if the current tesseract token is an error.

```{r data preparation,warning=FALSE}
# initialze variables
line_diff <- rep(NA, 100)
ground_truth_list <- list(); ground_truth_list_vec <- list()
tesseract_list <- list(); tesseract_list_vec <- list()
tokens_per_line_list <- list(); truth_tokens_per_line_list <- list()
error_list <- list(); nonerror_list <- list()
clean_ind_list <- list()
# for corrections
error_list_vec <- list()

for (i in c(1:length(file_name_vec))) {
  current_file_name <- file_name_vec[i]
  ## read the ground truth text
  current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,sep=""), warn=FALSE)
  ## for the current file, construct a list of vectors with each element representing one line
  current_ground_truth_list_vec <- lapply(current_ground_truth_txt, stringsplit_1st)
  ground_truth_list_vec[[i]] <- current_ground_truth_list_vec
  ## count the number of tokens in each line
  truth_tokens_per_line_list[[i]]  <- sapply(current_ground_truth_list_vec, length)
  ## create a vector including all the tokens in the current file
  ground_truth_list[[i]] <- unlist(current_ground_truth_list_vec)
  
  ## read the tesseract text: procedure similar to the ground truth files
  current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,sep=""), warn=FALSE)
  current_tesseract_list_vec <- lapply(current_tesseract_txt, stringsplit_1st)
  tesseract_list_vec[[i]] <- current_tesseract_list_vec
  tokens_per_line_list[[i]] <- sapply(current_tesseract_list_vec, length)
  current_tesseract_vec <- unlist(current_tesseract_list_vec)
  tesseract_list[[i]]<- current_tesseract_vec
  
  ## compare tesseract text and ground truth, basically matching them line by line
  match_result <- doc_match(current_tesseract_list_vec, current_ground_truth_list_vec)
  current_clean_ind <- unlist(match_result[[1]])
  clean_ind_list[[i]] <- current_clean_ind
  ## get the diffence in the number of lines between the tesseract file and the ground truth
  line_diff[i] <- match_result[[2]]
  error_list_vec[[i]] <- match_result[[3]]
  ## create a vector of errors and one of non-errors for the current tesseract file
  error_list[[i]] <- current_tesseract_vec[!current_clean_ind]
  nonerror_list[[i]] <- current_tesseract_vec[current_clean_ind]
}

# compute the ground truth mean number of tokens per line
truth_mean_tokens_per_line <- mean(sapply(truth_tokens_per_line_list, mean))
# do the same for tesseract
tesseract_mean_tokens_per_line <- mean(sapply(tokens_per_line_list, mean))
```




## Step 3.2 - Train SVM

### Step 3.2.1 - Training data preparation

We use 80 tesseract files from 5 groups as our training set.
```{r,warning=FALSE}
train_file_id <- c(3:10, 17:38, 40:41, 47:68, 75:100)
train_tesseract_list <- tesseract_list[train_file_id]
train_clean_ind_list <- clean_ind_list[train_file_id]
train_truth_list <- ground_truth_list[train_file_id]
train_truth <- unlist(train_truth_list)
train_truth_unique <- unique(train_truth)
train_ngrams <- prepare_ngram(train_tesseract_list)
train_freq_ngrams <- table(train_ngrams)

# compile a frequency list of letter bigrams
train_truth_bigrams <- tolower(unlist(lapply(train_truth, bigram_from_token)))
train_freq_bigrams <- table(train_truth_bigrams)

time_trainfeat <- system.time(train_input <- extract_feature(train_tesseract_list, train_truth_unique, train_freq_ngrams, train_freq_bigrams))
train_labels <- unlist(train_clean_ind_list)

save(train_input, file="../output/feature_train.RData")
```


### Step 3.2.2 - Cross validation

Considering that an undetected error will not get into the correction phase, we evaluate error detection as a recall oriented task, which focus more on finding all possible errors. Here we perform a 5-fold cross validation for the svm model using recall as evaluation metric. The tuning parameter is the bandwidth for the Gaussian kernel (as stated in the paper), sigma.

```{r cross validation}
set.seed(1)
K <- 5
# 5-fold cross validation for tuning the bandwith parameter sigma
sigmas <- c(0.01, 0.1, 1)
cv <- perform_cv(train_input, train_labels, sigmas, K)
save(cv, file="../output/cv_result.RData")
```

### Step 3.2.3 - Train SVM model

According to the results of cross-validation, the best sigma is 0.1. Now we can train the SVM model based on 80 training files.

```{r model fitting}
set.seed(1)
load("../output/cv_result.RData")
cv_result <- cv$cv_result
best_sigma <- cv$best_sigma

# train the svm model
time_trainsvm <- (system.time(
  fit_svm <- ksvm(train_input, as.factor(train_labels), type = "C-svc", kernel = "rbfdot", 
                  kpar = list(sigma = best_sigma))))

save(fit_svm, time_trainsvm, file="../output/fit_svm.RData")
```

## Step 3.3 - Test SVM

We use the rest 20 tesseract files of tokens as the test data.

```{r test svm}
test_file_id <- c(1:2, 11:16,39,42:46, 69:74)
# lists of test tokens
test_tokens_list <- tesseract_list[test_file_id]
num_test_tokens_list <- length(unlist(test_tokens_list)) #51866 tokens
# labels are logical TRUE/FALSE
test_labels_list <- clean_ind_list[test_file_id]
test_labels <- unlist(test_labels_list)
# make a list of matrices for the input features instead of collapsing them into one matrix 
# for future document match on the corrected errors


test_truth_list <- ground_truth_list[test_file_id]
num_test_truth_list <- length(unlist(test_truth_list)) #51218 tokens
train_truth <- unlist(train_truth_list)
train_truth_unique <- unique(train_truth)
train_ngrams <- prepare_ngram(train_tesseract_list)
train_freq_ngrams <- table(train_ngrams)

test_ngrams <- prepare_ngram(test_tokens_list)
test_freq_ngrams <- table(test_ngrams)

time_trainfeat <- system.time(train_input <- extract_feature(train_tesseract_list, train_truth_unique, train_freq_ngrams, train_freq_bigrams))

time_testfeat <- system.time(test_input <- extract_feature(test_tokens_list, train_truth_unique, test_freq_ngrams, train_freq_bigrams))
preds <- as.logical(predict(fit_svm, test_input))
save(preds, time_testfeat, test_input, file="../output/predict_svm.RData")
```


# Step 4 - Error correction

Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates. Here we compute several features for each correciton candidate, and use regression-based methods for candidate ranking.

The referenced papers are:

[Statistical Learning for OCR Text Correction](https://arxiv.org/pdf/1611.06950.pdf)

[OCR Post-Processing Text Correction using Simulated Annealing](http://aclweb.org/anthology/U17-1015)

The relevant features are:

1) Minimum edit distance $d(w_{ci} , w_e )$ as calculated by Levenshtein's edit distance.

2) Normalized longest common subsequence which takes into account the length of both the shorter and the longer string for normalization.

$nlcs(w_{ci} , w_e ) =\frac{2*len(lcs(w_{ci} , w_e ))}{len(w_{ci}) + len(w_e)}$

There are three types of modifications with different additional conditions: $\textit{NLCS}_1$ and $\textit{NLCS}_n$ use the subsequences starting at the first and the n-th character, respectively; $\textit{NLCS}_z$ takes the subsequences ending at the last character. They apply the same normalization as $\textit{NLCS}$.

$nmnlcs_1(w_c , w_e ) =\frac{2 * len(mclcs_1(w_c , w_e ))}{len(w_c) + len(w_e)}$

$nmnlcs_n(w_c , w_e ) =\frac{2 * len(mclcs_n(w_c , w_e ))}{len(w_c) + len(w_e)}$

$nmnlcs_z(w_c , w_e ) =\frac{2 * len(mclcs_z(w_c , w_e ))}{len(w_c) + len(w_e)}$

We use the weighted average of these four.

3) Language popularity

Using a language lexicon is a common approach to detect the non-word tokens,
where non-existing tokens are detected as true errors. Let $w_c$ be the candidate string, $\mathcal{C}$ be the set of all error candidates, and $freq_1(??)$ be the unigram frequency. The candidate confidence is the unigram popularity given by:

$score(w_c, w_e) = \frac{freq_1(w_c)}{max_{w'_c \in C}freq_1(w'_c)}$


4) Lexicon Existance

Besides English lexicon, we can use different lexicons to detect the existence of the token in different subjects. It identifies additional lexical features. For example, we may use a domain specific lexicon to capture terminologies, which is especially useful for input text from the same domain. The candidate selection is the same as English lexicon, but the candidate score is a boolean value that indicates the detection result.

$score(w_c, w_e) = \begin{cases} 1 & \text{if $w_c$ exists in the lexicon}\\ 0 & \text{otherwise} \end{cases}$

5) Context popularity

An appropriate correction candidate should be coherent in context. Using word n-gram for context analysis is a broadly researched approach in correcting real word errors (Islam and Inkpen, 2009a). Given an error word $w_e$ in a text, we have its n-gram contexts $\mathcal{G}$ constructed using a sliding window. To score a candidate $w_c$ of this error, we first substitute the error word from each of its n-gram contexts by such candidate and create a new set of contexts $\mathcal{G}_c$. Let $\mathcal{C}$ be all candidates suggested for $w_e$, and $freq_n(??)$ be the n-gram frequency, which gives 0 to a non-existing n-gram.
The score function is given as:

$score(w_c, w_e) = \frac{\Sigma_{\textbf{c} \in \mathcal{G}_c} freq_n(\textbf{c})}{max_{w'_c \in \mathcal{C}}\{\Sigma_{\textbf{c'} \in \mathcal{G'}_c} freq_n(\textbf{c'})\}}$

+ Input: error tokens in train set and test set
+ Output: trained model&corrections for detected errors in the test set 

```{r}
#### function to generate features for every proposed corrections
source('../lib/score_components.R')
```

## Step 4.1 - Data preperation

Since transposition errors are common in human-generated text but rarely occur in the OCR-generated text, we apply Levenshtein distance (Levenshtein, 1966), which uses a simpler operation set without transposition to choose candidates.

We formulate the confidence prediction task as a regression problem. Given candidate feature scores, we predict the confidence of each candidate being a correction for the error word. The confidence is used for ranking among candidates of one error. To train a regressor for correction, we label candidate features with 1 if a candidate is the intended correction, or 0 otherwise. The training data contains candidates from different errors, and there are more candidates labeled 0 than 1. 
```{r, warning=FALSE}
# load data
ground_truth_txt <- readLines("../output/train_truth.txt", warn=FALSE)
ground_truth_list_vec <- lapply(ground_truth_txt, stringsplit_1st)
ground_truth_list <- unlist(ground_truth_list_vec)

train_data <- read.csv('../output/train_error.csv')

# Use training ground truth to create n-gram context
n_gram <- 3
context_ground_truth <- list()
for (i in 1:(length(ground_truth_list)-(n_gram-1))){
  context_name <- paste(ground_truth_list[(i):(i+n_gram-1)], collapse = " ")
  context_ground_truth[[i]] <- context_name
}
context_3gram_freq <- list(ngram = unlist(unique(context_ground_truth)), 
                           count = tabulate(match(context_ground_truth, unique(context_ground_truth))))

# set Levenshtein distance threshold
thres <- 2

ground_truth_list_clean <- 'first'
for (each in unique(ground_truth_list)){
  each_clean <- stri_extract_all_words(each)[[1]]
  ground_truth_list_clean <- c(ground_truth_list_clean, each_clean)
}
# all_words <- unique(c(english.words, tolower(na.omit(ground_truth_list_clean))))
all_words <- unique(c(english.words, tolower(na.omit(ground_truth_list))))

# create the training features and labels
#### here, score_features is a function to generate features for every proposed candidte
cnt <- 0
for (i in 1:nrow(train_data)){
  if (i %% 1000 == 1){
    print(i)
  }
  candidates <- unlist(levenshtein.neighbors(as.character(train_data$c)[i], all_words)[1:thres])
  if (length(candidates)>0){
    cnt <- cnt + 1
    feature_i <- score_features(wc=candidates, we=as.character(train_data$c)[i], thres=2, lexicon=ground_truth_list, 
                             context_we=as.character(data.frame(lapply(train_data[i,2:6], as.character), stringsAsFactors=FALSE)), 
                             context_3grams_freq=context_3gram_freq)
    feature_i$label <- as.numeric(candidates == as.character(train_data$Truth)[i])
    if (cnt==1){
      feature_scores <- feature_i
    } else{
      feature_scores <- rbind(feature_scores, feature_i)
    }
  }
}

is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.nan))
feature_scores[is.nan(feature_scores)] <- 0

### save features for training data
feat_train <- data.matrix(feature_scores[,1:5])
label_train <- data.matrix(feature_scores$label)

save(feature_scores, file="../output/train_correction.RData")
```

## Step 4.2 - Train the model with score features


### Model selection with cross-validation

```{r}
### parameter for cv ( 5 floders)
run.cv <- TRUE
K <- 5
```


#### xgboost
```{r, message=FALSE, warning=FALSE}
#### cv, train and test function for xgboost
source("../lib/xgb_cv.R")
source("../lib/xgb_train.R")
source("../lib/xgb_test.R")
load("../output/train_correction.RData")
feat_train <- data.matrix(feature_scores[,1:5])
label_train <- data.matrix(feature_scores$label)

### parameter to choose
depth_values <- seq(3, 6, 1)

if(run.cv){
   start_time <- Sys.time()
   xgb_err_cv <- array(dim=c(length(depth_values), 2))
   for(k in 1:length(depth_values)){
     cat("k=", k, "\n")
     xgb_err_cv[k,] <- xgb_cv.function(feat_train, label_train, depth_values[k], K)
   }
   end_time <- Sys.time()
   tm_xgb_cv <- end_time - start_time
  # print(tm_xgb_cv)
   save(xgb_err_cv, file="../output/xgb_err_cv.RData")
  }
```

Visualize the cv results.
```{r}
if(run.cv){
  load("../output/xgb_err_cv.RData")
  plot(depth_values, xgb_err_cv[,1], xlab="Depth", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0, 0.02))
  points(depth_values, xgb_err_cv[,1], col="blue", pch=16)
  lines(depth_values, xgb_err_cv[,1], col="blue")
  arrows(depth_values, xgb_err_cv[,1]-xgb_err_cv[,2], depth_values, xgb_err_cv[,1]+xgb_err_cv[, 2], 
        length=0.1, angle=90, code=3)
}
```

* Choose the "best" parameter value
```{r}
if(run.cv){
  xgb_model_best <- depth_values[which.min(xgb_err_cv[,1])]
}
par_best_xgb <- list(depth=xgb_model_best)
```

#### adaboost

```{r, message=FALSE, warning=FALSE}
#### cv function for adaboost using linear loss and decision tree as the weak classifer
source("../lib/adaboostM1_cv.R")
load("../output/train_correction.RData")
feat_train1 <- data.matrix(feature_scores[,1:5])

label_train1 <- data.matrix(ifelse(feature_scores$label==0,-1,1))

#### parameter to choose: tree depth in decisino tree
tree_depth <- seq(2,4,1)

if(run.cv){
   start_time <- Sys.time()
   adb_err_cv <- array(dim=c(length(tree_depth), 2))
   for(k in 1:length(tree_depth)){
     cat("k=", k, "\n")
     adb_err_cv[k,] <- adaboostcv.function(feat_train1, label_train1, tree_depth[k], K)
   }
   end_time <- Sys.time()
   tm_adb_cv <- end_time - start_time
  # print(tm_adb_cv)
   save(adb_err_cv, file="../output/adb_err_cv.RData")
  }
```

Visualize the cv results.
```{r}
if(run.cv){
  load("../output/adb_err_cv.RData")
  plot(tree_depth, adb_err_cv[,1], xlab="Tree Depth", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0, 0.02))
  points(tree_depth, adb_err_cv[,1], col="blue", pch=16)
  lines(tree_depth, adb_err_cv[,1], col="blue")
  arrows(tree_depth, adb_err_cv[,1]-adb_err_cv[,2], tree_depth, adb_err_cv[,1]+adb_err_cv[, 2], 
        length=0.1, angle=90, code=3)
}
```

* Choose the "best" parameter value
```{r}
if(run.cv){
  adb_model_best <- tree_depth[which.min(adb_err_cv[,1])]
}
par_best_adb <- list(depth=adb_model_best)
```



### Train the model with entire training set 

#### xgboost

```{r}
source("../lib/xgb_train.R")
source("../lib/xgb_test.R")
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r}
tm_train_xgb=NA
tm_train_xgb <- system.time(fit_train_xgb <- xgb_train(feat_train, label_train, par_best_xgb))
save(fit_train_xgb, file="../output/fit_train_xgb.RData")
# load(file="../output/fit_train_xgb.RData")
```

* Feature importance
```{r}
importance <- xgb.importance(feature_names = colnames(feature_scores)[1:5], model = fit_train_xgb)
print(importance)
```

#### adboost

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r}
tm_train_adb=NA
tm_train_adb <- system.time(fit_train_adb <- adaboost(feat_train1, label_train1, par_best_adb))
save(fit_train_adb, file="../output/fit_train_adb.RData")
# load(file="../output/fit_train_adb.RData")
```



## Step 4.3 - Make predictions on test set

### First extract the features.
```{r, warning=FALSE}
test_data <- read.csv('../output/test_error.csv')

thres1 <- 2
# create the test features and labels
cnt1 <- 0
for (i in 1:nrow(test_data)){
  if (i %% 1000 == 1){
    print(i)
  }
  candidates <- unlist(levenshtein.neighbors(as.character(test_data$c)[i], all_words)[1:thres1])
  if (length(candidates)>0){
    cnt1 <- cnt1 + 1
    feature_i <- score_features(wc=candidates, we=as.character(test_data$c)[i], thres=2, lexicon=ground_truth_list, 
                             context_we=as.character(data.frame(lapply(test_data[i,2:6], as.character), stringsAsFactors=FALSE)), 
                             context_3grams_freq=context_3gram_freq)
    feature_i$label <- as.numeric(candidates == as.character(test_data$Truth)[i])
    feature_i$tess <- as.character(test_data[i,'tess'])
    feature_i$cand <- candidates
    feature_i$truth <- as.character(test_data[i,'Truth'])
    if (cnt1==1){
      feature_scores_test <- feature_i
    } else{
      feature_scores_test <- rbind(feature_scores_test, feature_i)
    }
  }
}

is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.nan))
feature_scores_test[is.nan(feature_scores_test)] <- 0

### save features for test data
feat_test <- data.matrix(feature_scores_test[,1:5])
label_test <- data.matrix(feature_scores_test$label)

save(feature_scores_test, file="../output/test_correction.RData")
```

### Then predict the scores and choose the best correction.

#### xgboost


```{r}
load("../output/test_correction.RData")
feat_test <- data.matrix(feature_scores_test[,1:5])
label_test <- data.matrix(feature_scores_test$label)
```



```{r}
### for every unique error token, provide the correction token with highest probability
predicted_score <- xgb_test(fit_train_xgb, feat_test)
cnt2 <- 0
for (each in unique(feature_scores_test$tess)){
  cnt2 <- cnt2 + 1
  each_index <- which(feature_scores_test$tess == each)
  test_each <- feature_scores_test[each_index,]
  max_index <- which.max(predicted_score[each_index])
  correction_each <- test_each[max_index, 'cand']
  truth_each <- test_each[max_index, 'truth']
  if (cnt2==1){
    correction <- correction_each
    test_truth <- truth_each
  } else{
    correction <- c(correction, correction_each)
    test_truth <- c(test_truth, truth_each)
  }
}

```

#### adaboost

```{r}
load("../output/test_correction.RData")
feat_test1 <- data.matrix(feature_scores_test[,1:5])

label_test1 <- data.matrix(ifelse(feature_scores_test$label==0,-1,1))

```



```{r}
### for every unique error token, provide the correction token with highest probability
predicted_score <- predict(fit_train_adb, feat_test1,type="prob")
cnt2 <- 0
for (each in unique(feature_scores_test$tess)){
  cnt2 <- cnt2 + 1
  each_index <- which(feature_scores_test$tess == each)
  test_each <- feature_scores_test[each_index,]
  max_index <- which.max(predicted_score[each_index])
  correction_each <- test_each[max_index, 'cand']
  truth_each <- test_each[max_index, 'truth']
  if (cnt2==1){
    correction1 <- correction_each
    test_truth1 <- truth_each
  } else{
    correction1 <- c(correction1, correction_each)
    test_truth1 <- c(test_truth1, truth_each)
  }
}


```

```{r}
#### find out the number of every unique error tokens in order to generate a correction list with the samle length of test list
numberforerror <- rep(NA,length(unique(feature_scores_test$tess)))
for (i in 1:length(unique(feature_scores_test$tess))){
  numberforerror[i] <- sum(unique(feature_scores_test$tess)[i]==test_data$tess)
}
correction_xgb <- rep(correction,numberforerror)
test_truth_xgb <- rep(test_truth,numberforerror)

correction_adb <- rep(correction1,numberforerror)
test_truth_adb <- rep(test_truth1,numberforerror)
```

# Step 5 - Performance measure

## Step 5.1 - Evaluation on detection performance measure

Considering that an undetected error will not get into the correction phase, we evaluate error detection as a recall oriented task, which focus more on finding all possible errors. Below is the confusion matrix for error detection. To evaluate the performance of error detection, we define

\begin{align*} 
\mbox{detection recall}&=\frac{\mbox{number of correctly detected errors}}{\mbox{number of actual errors}}\\
\mbox{detection precision}&=\frac{\mbox{number of correctly detected errors}}{\mbox{number of detected errors}}
\end{align*}

```{r error matrix}
# relabel the actual correctness and model selection results
test_labels <- unlist(test_labels_list)
model_detection <- factor(ifelse(preds, "Correct","Error" ), levels = c("Error", "Correct"))
actual_correctness <- factor(ifelse(test_labels, "Correct","Error" ), levels = c("Error", "Correct"))
# create confusion matrix
detection_confusion_matrix <- table(actual_correctness,model_detection)
detection_confusion_matrix_frac <- round(detection_confusion_matrix/length(model_detection),4)
# calculate recall and precision
detection_recall <- detection_confusion_matrix[1,1]/sum(detection_confusion_matrix[1,])
detection_precision <- detection_confusion_matrix[1,1]/sum(detection_confusion_matrix[,1])
detection_recall_display <- paste(round(100*detection_recall,4), "%", sep  = "")
detection_precision_display <- paste(round(100*detection_precision,4), "%", sep  = "")

# display confusion matrix
detection_confusion_matrix
# display detection recall and precision 
cat(" detection recall = ", detection_recall_display, "\n",
    "detection precision = ", detection_precision_display, "\n")
```

## Step 5.2 - Evaluation on correction performance measure

### xgboost

The correction accuracy strongly depends on the threshold of Levenshtein distance. For instance, when we take threshold = 1, the result is shown below.
```{r}
accuracy_with_cand <- sum(correction_xgb == test_truth_xgb)/length(correction_xgb)
print(accuracy_with_cand)

overall_accuracy <- sum(correction_xgb == test_truth_xgb)/nrow(test_data)
print(overall_accuracy)
```
### adaboost
```{r}
accuracy_with_cand <- sum(correction_adb == test_truth_adb)/length(correction_adb)
print(accuracy_with_cand)

overall_accuracy <- sum(correction_adb == test_truth_adb)/nrow(test_data)
print(overall_accuracy)
```


## Step 5.3 - Performance Evaluation

The two most common OCR accuracy measures are precision and recall. Both are relative measures of the OCR accuracy because they are computed as ratios of the correct output to the total output (precision) or input (recall). More formally defined,
\begin{align*}
\mbox{precision}&=\frac{\mbox{number of correct items}}{\mbox{number of items in OCR output}}\\
\mbox{recall}&=\frac{\mbox{number of correct items}}{\mbox{number of items in ground truth}}
\end{align*}
where *items* refer to either characters or words, and ground truth is the original text stored in the plain text file. 

Both *precision* and *recall* are mathematically convenient measures because their numeric values are some decimal fractions in the range between 0.0 and 1.0, and thus can be written as percentages. For instance, recall is the percentage of words in the original text correctly found by the OCR engine, whereas precision is the percentage of correctly found words with respect to the total word count of the OCR output. Note that in the OCR-related literature, the term OCR accuracy often refers to recall.

```{r}

alltest <- read.csv("../output/test_error_new.csv")

# word level evaluation
# old interaction
old_intersect_vec <- vecsets::vintersect(alltest$Truth,alltest$tess)
# new interaction
#### here we use the correction result from xgboost because it has higher accuracy acoording to step 5.2
new_intersect_vec <- vecsets::vintersect(correction,  test_truth)
new_intersect_vec2 <- c(old_intersect_vec, new_intersect_vec)

# character-level evaluation
# old interaction
ground_truth_vec_char <- str_split(paste(unlist(test_truth_list), collapse = ""), "")[[1]]
tesseract_vec_char <- str_split(paste(unlist(test_tokens_list), collapse = ""), "")[[1]]
old_intersect_vec_char <- str_split(paste(unlist(old_intersect_vec), collapse = ""), "")[[1]]
# new interaction
# function used to compare pairs of words in character level

new_intersect_vec_char <- str_split(paste(unlist(new_intersect_vec2), collapse = ""), "")[[1]]

length(new_intersect_vec_char)/length(ground_truth_vec_char)
length(new_intersect_vec_char)/length(tesseract_vec_char)
length(new_intersect_vec2)/num_test_truth_list
length(new_intersect_vec2)/num_test_tokens_list

```


